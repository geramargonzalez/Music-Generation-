# üéµ Generaci√≥n de M√∫sica con Deep Learning (RNN/LSTM)

**Proyecto acad√©mico ‚Äî Curso de Deep Learning | UTEC √ó MIT**  
Especializaci√≥n en Ciencia de Datos y Machine Learning

**Autor:** Gerardo Gonz√°lez

---

## Descripci√≥n

Este proyecto implementa un modelo de **Red Neuronal Recurrente (RNN)** basado en la arquitectura **LSTM (Long Short-Term Memory)** para la **generaci√≥n autom√°tica de m√∫sica**. El modelo aprende patrones a partir de partituras codificadas en [notaci√≥n ABC](https://en.wikipedia.org/wiki/ABC_notation) y, una vez entrenado, es capaz de componer nuevas melod√≠as de forma aut√≥noma.

El notebook est√° basado en el **Lab 1 ‚Äì Part 2** del curso [MIT Introduction to Deep Learning (6.S191)](http://introtodeeplearning.com), adaptado y documentado como proyecto de la especializaci√≥n en Ciencia de Datos y Machine Learning de la **UTEC**.

---

## Estructura del Proyecto

```
DeepMusic Generation/
‚îú‚îÄ‚îÄ Gerardo_Gonzalez_Music_Generation.ipynb   # Notebook principal del proyecto
‚îú‚îÄ‚îÄ Gerardo_Gonzalez_Music_Generation_vc.ipynb # Versi√≥n de control del notebook
‚îú‚îÄ‚îÄ deepMusicGeneration.ipynb                  # Notebook exploratorio
‚îú‚îÄ‚îÄ PT_Part1_Intro.ipynb                       # Parte 1: Introducci√≥n a PyTorch
‚îú‚îÄ‚îÄ training_checkpoints/                      # Checkpoints del modelo entrenado
‚îú‚îÄ‚îÄ output_*.wav                               # Archivos de audio generados
‚îî‚îÄ‚îÄ README.md                                  # Este archivo
```

---

## Pipeline del Proyecto

### 1. Configuraci√≥n del Entorno
- Instalaci√≥n de dependencias: **PyTorch**, **Comet ML** (tracking de experimentos), **mitdeeplearning** (utilidades del laboratorio MIT).
- Herramientas de sistema para s√≠ntesis de audio: `abcmidi` (ABC ‚Üí MIDI) y `timidity` (MIDI ‚Üí WAV).

### 2. Carga y Exploraci√≥n del Dataset
- Se carga un corpus de canciones en **notaci√≥n ABC** mediante `mitdeeplearning`.
- Se puede escuchar cualquier canci√≥n del dataset convirti√©ndola a audio.

### 3. Preprocesamiento
- **Tokenizaci√≥n a nivel de caracter:** se construye un vocabulario con todos los caracteres √∫nicos del corpus (~83 caracteres).
- **Mapeos** `char2idx` / `idx2char` para convertir entre texto y representaci√≥n num√©rica.
- **Vectorizaci√≥n** del texto completo del dataset.
- **Generaci√≥n de batches:** pares (input, target) donde el target es la secuencia desplazada un caracter a la derecha (predicci√≥n del siguiente caracter).

### 4. Arquitectura del Modelo (LSTM)

| Capa | Descripci√≥n | Dimensi√≥n |
|------|-------------|-----------|
| `nn.Embedding` | Convierte IDs de caracteres en vectores densos aprendidos | `vocab_size ‚Üí 256` |
| `nn.LSTM` | Procesa la secuencia manteniendo memoria temporal | `256 ‚Üí 1024` |
| `nn.Linear` | Proyecta los estados ocultos a logits sobre el vocabulario | `1024 ‚Üí vocab_size` |

**Salida del modelo:** `(batch_size, seq_length, vocab_size)` ‚Äî logits para cada posici√≥n temporal.

### 5. Entrenamiento

- **Funci√≥n de p√©rdida:** `CrossEntropyLoss` (clasificaci√≥n multi-clase caracter por caracter).
- **Optimizador:** Adam con `learning_rate = 2e-3`.
- **Hiperpar√°metros principales:**

| Par√°metro | Valor |
|-----------|-------|
| Iteraciones de entrenamiento | 6,000 |
| Tama√±o de batch | 32 |
| Longitud de secuencia | 200 |
| Learning rate | 2e-3 |
| Dimensi√≥n de embedding | 256 |
| Tama√±o oculto LSTM | 1024 |

- **Tracking:** m√©tricas de p√©rdida registradas en [Comet ML](https://www.comet.com/) para monitoreo en tiempo real.
- **Checkpoints:** guardados cada 100 iteraciones.

### 6. Generaci√≥n de M√∫sica

Se implementaron dos t√©cnicas de muestreo para la generaci√≥n de texto:

- **Muestreo multinomial est√°ndar:** basado en softmax sobre los logits.
- **Muestreo Nucleus (Top-p) con temperatura:** filtra los caracteres menos probables reteniendo solo el n√∫cleo de probabilidad acumulada hasta `p`, y controla la aleatoriedad con un par√°metro de temperatura.
  - `temperature < 1.0` ‚Üí generaci√≥n m√°s conservadora y v√°lida.
  - `temperature > 1.0` ‚Üí generaci√≥n m√°s creativa pero con mayor riesgo de errores sint√°cticos.

El texto generado en notaci√≥n ABC se convierte a audio (MIDI ‚Üí WAV) para su reproducci√≥n.

### 7. Resultados

El modelo entrenado genera m√∫ltiples canciones con estructura ABC v√°lida, incluyendo:
- Headers correctos (`X:`, `T:`, `M:`, `L:`, `K:`)
- Barras de comp√°s (`|`, `:|`)
- Patrones mel√≥dicos y r√≠tmicos coherentes

**Canciones generadas de ejemplo:**

| Canci√≥n | Duraci√≥n | Tempo estimado | Timbre (centroide espectral) |
|---------|----------|----------------|------------------------------|
| Song 0 | ~28.1 s | ~120 BPM (r√°pido) | ~3397 Hz (brillante) |
| Song 1 | ~67.7 s | ~60 BPM (lento) | ~2947 Hz (c√°lido/oscuro) |

---

## Tecnolog√≠as Utilizadas

- **Python 3**
- **PyTorch** ‚Äî framework de deep learning
- **Comet ML** ‚Äî tracking de experimentos
- **NumPy** ‚Äî manipulaci√≥n num√©rica
- **SciPy** ‚Äî escritura de archivos WAV
- **tqdm** ‚Äî barras de progreso
- **mitdeeplearning** ‚Äî utilidades del curso MIT 6.S191
- **abcmidi / timidity** ‚Äî conversi√≥n ABC ‚Üí MIDI ‚Üí Audio

---

## C√≥mo Ejecutar

1. **Abrir el notebook** `Gerardo_Gonzalez_Music_Generation.ipynb` en Google Colab o en un entorno local con GPU.
2. **Configurar la API key de Comet ML** (registrarse en [comet.com](https://www.comet.com/) y obtener una clave personal).
3. **Ejecutar todas las celdas** en orden secuencial.
4. Los archivos de audio generados (`output_*.wav`) se guardar√°n localmente y se registrar√°n en Comet ML.

> **Nota:** Se recomienda usar un entorno con GPU (Google Colab con runtime GPU) para acelerar el entrenamiento.

---

## Contexto Acad√©mico

| | |
|---|---|
| **Instituci√≥n** | UTEC (Universidad de Ingenier√≠a y Tecnolog√≠a) |
| **Curso** | Deep Learning (MIT Introduction to Deep Learning ‚Äî 6.S191) |
| **Especializaci√≥n** | Ciencia de Datos y Machine Learning |
| **Laboratorio** | Lab 1, Parte 2 ‚Äî Generaci√≥n de M√∫sica con RNNs |
| **Autor** | Gerardo Gonz√°lez |

---

## Referencias

- [MIT Introduction to Deep Learning (6.S191)](http://introtodeeplearning.com)
- [Repositorio del curso en GitHub](https://github.com/MITDeepLearning/introtodeeplearning)
- [Documentaci√≥n de PyTorch](https://pytorch.org/docs/stable/)
- [Notaci√≥n ABC ‚Äî Wikipedia](https://en.wikipedia.org/wiki/ABC_notation)
- [Comet ML ‚Äî Documentaci√≥n](https://www.comet.com/docs/v2/)

---

## Licencia

¬© MIT Introduction to Deep Learning ‚Äî [http://introtodeeplearning.com](http://introtodeeplearning.com)
